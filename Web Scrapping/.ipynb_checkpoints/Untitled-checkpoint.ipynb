{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "for num in range(1,11):    \n",
    "    scrap_url = 'http://quotes.toscrape.com/page/{}'.format(num)\n",
    "    urlClnt = urlopen(scrap_url)\n",
    "    raw_html = urlClnt.read()\n",
    "    urlClnt.close\n",
    "\n",
    "    page = soup(raw_html, 'html.parser')\n",
    "    containers = page.findAll('div', {'class' : 'quote'})\n",
    "    container = containers[0]\n",
    "\n",
    "    with open('scrapping.csv', 'a+', encoding=\"utf8\") as f:\n",
    "        f.write('Quote,Author,Tags\\n')\n",
    "\n",
    "        for container in containers:\n",
    "            Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "            Author = container.find('small',{'class' : 'author'}).text\n",
    "            Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "\n",
    "            f.write(Quote + ',' + Author + ',' + Tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://quotes.toscrape.com/page/2/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "scrap_url = 'http://quotes.toscrape.com/page/3/'\n",
    "urlClnt = urlopen(scrap_url)\n",
    "raw_html = urlClnt.read()\n",
    "urlClnt.close\n",
    "\n",
    "page = soup(raw_html, 'html.parser')\n",
    "containers = page.findAll('div', {'class' : 'quote'})\n",
    "container = containers[0]\n",
    "\n",
    "with open('test.csv', 'w') as f:\n",
    "    f.write('Quote,Author,Tags\\n')\n",
    "    \n",
    "    for container in containers:\n",
    "        Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "        Author = container.find('small',{'class' : 'author'}).text\n",
    "        Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "        \n",
    "        f.write(Quote + ',' + Author + ',' + Tags + '\\n')\n",
    "\n",
    "navigator = page.findAll('div', {'class' : 'col-md-8'})\n",
    "navigat = navigator[1]\n",
    "u = navigat.nav.a['href']\n",
    "# len(u)\n",
    "url = 'http://quotes.toscrape.com' + u\n",
    "url\n",
    "scrap_url = url\n",
    "scrap_url   \n",
    "\n",
    "\n",
    "# What i want to do now:\n",
    "\n",
    "# I have got a new url in the 'scrap_url' variable in the last line. Now, I want to send the new 'scrap_url' \n",
    "# to the beginning 'scrap_url' and update the beginning 'scrap_url' with new value and \n",
    "# continue the whole process as long as we get different value for 'scrap_url'(at the end) and 'scrap_url'(at the beginning)\n",
    "\n",
    "# If we can loop over this process then we will use 'append' mode in file instead of 'write mode'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "for num in range(1,15):\n",
    "    res = requests.get('http://www.abcde.com/Part?Page={num}&s=9&type=%8172653').text\n",
    "    soup = BeautifulSoup(res,\"lxml\")\n",
    "    for item in soup.select(\".article-title\"):\n",
    "        print(urljoin('http://www.abcde.com',item['href']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A day without sunshine is like you know night.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = '{\"a\": \"1\", \"b\": \"2\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{a: 1, b: 2}'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quotes.toscrape.com/page/1\n",
      "http://quotes.toscrape.com/page/2\n",
      "http://quotes.toscrape.com/page/3\n",
      "http://quotes.toscrape.com/page/4\n",
      "http://quotes.toscrape.com/page/5\n",
      "http://quotes.toscrape.com/page/6\n",
      "http://quotes.toscrape.com/page/7\n",
      "http://quotes.toscrape.com/page/8\n",
      "http://quotes.toscrape.com/page/9\n",
      "http://quotes.toscrape.com/page/10\n"
     ]
    }
   ],
   "source": [
    "for num in range(1,11):\n",
    "    scrap_url = 'http://quotes.toscrape.com/page/{}'.format(num)\n",
    "    print(scrap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://quotes.toscrape.com/page/['1', '2', '3', '4']\n",
      "['1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "pages = [str(num) for num in range(1,5)]\n",
    "scrap_url = 'http://quotes.toscrape.com/page/{}'.format(pages)\n",
    "print(scrap_url)\n",
    "print(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "for num in range(1,11):    \n",
    "    scrap_url = 'http://quotes.toscrape.com/page/{}'.format(num)\n",
    "    urlClnt = urlopen(scrap_url)\n",
    "    raw_html = urlClnt.read()\n",
    "    urlClnt.close\n",
    "\n",
    "    page = soup(raw_html, 'html.parser')\n",
    "    containers = page.findAll('div', {'class' : 'quote'})\n",
    "    container = containers[0]\n",
    "\n",
    "    with open('scrapping.csv', 'a', encoding=\"utf8\") as f:\n",
    "        f.write('Quote,Author,Tags\\n')\n",
    "\n",
    "        for container in containers:\n",
    "            Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "            Author = container.find('small',{'class' : 'author'}).text\n",
    "            Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "\n",
    "            f.write(Quote + ',' + Author + ',' + Tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    # Author: Mehedi Afzal Farazi\n",
    "#Problem:\n",
    "#                               Scrape all data from this url \"http://quotes.toscrape.com/\" i.e. Home page\n",
    "#                               And dump the scrape data in a csv file.\n",
    "#                               Column Header will be Quote, Author, Tags.\n",
    "#                               All the tags will be in one cell and they will be separated by hifen \"-\"\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "scrap_url = 'http://quotes.toscrape.com/'\n",
    "urlClnt = urlopen(scrap_url)\n",
    "raw_html = urlClnt.read()\n",
    "urlClnt.close\n",
    "\n",
    "page = soup(raw_html, 'html.parser')\n",
    "containers = page.findAll('div', {'class' : 'quote'})\n",
    "container = containers[0]\n",
    "\n",
    "with open('scrapping.csv', 'w') as f:\n",
    "    f.write('Quote,Author,Tags\\n')\n",
    "    \n",
    "    for container in containers:\n",
    "        Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "        Author = container.find('small',{'class' : 'author'}).text\n",
    "        Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "        \n",
    "        f.write(Quote + ',' + Author + ',' + Tags + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                    # Author: Mehedi Afzal Farazi\n",
    "#Problem:\n",
    "#                               Scrape all data from this url \"http://quotes.toscrape.com/\" from all the pages\n",
    "#                               And dump the scrape data in a csv file.\n",
    "#                               Column Header will be Quote, Author, Tags.\n",
    "#                               All the tags will be in one cell and they will be separated by hifen \"-\"\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "for num in range(1,11):    \n",
    "    scrap_url = 'http://quotes.toscrape.com/page/{}'.format(num)\n",
    "    urlClnt = urlopen(scrap_url)\n",
    "    raw_html = urlClnt.read()\n",
    "    urlClnt.close\n",
    "\n",
    "    page = soup(raw_html, 'html.parser')\n",
    "    containers = page.findAll('div', {'class' : 'quote'})\n",
    "    container = containers[0]\n",
    "\n",
    "    with open('scrapping.csv', 'a', encoding=\"utf8\") as f:          \n",
    "        f.write('Quote,Author,Tags\\n')\n",
    "\n",
    "        for container in containers:\n",
    "            Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "            Author = container.find('small',{'class' : 'author'}).text\n",
    "            Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "\n",
    "            f.write(Quote + ',' + Author + ',' + Tags + '\\n')\n",
    "\n",
    "\n",
    "\n",
    "# Some comments:\n",
    "#   * At line 12 and 13 we are generating url for every page and loop through every page. \n",
    "\n",
    "#   * At line 22 we have to give (encoding=\"utf8\") because if we do not give it, \n",
    "#     for webpage 8 we will get (UnicodeDecodeError: 'charmap' codec can't decode) \n",
    "\n",
    "#   * We are using file mode 'append' in line 22 for writing the data. We we give 'write' mode\n",
    "#     then it will scrap all the pages and finally stores page 10 data after final overwrite in the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "for num in range(1,11):    \n",
    "    scrap_url = 'http://quotes.toscrape.com/page/{}'.format(num)\n",
    "    urlClnt = urlopen(scrap_url)\n",
    "    raw_html = urlClnt.read()\n",
    "    urlClnt.close\n",
    "\n",
    "    page = soup(raw_html, 'html.parser')\n",
    "#     navigator = page.findAll('ul', {'class' : 'pager'}\n",
    "    containers = page.findAll('div', {'class' : 'quote'})\n",
    "    container = containers[0]\n",
    "\n",
    "    with open('test.csv', 'a', encoding=\"utf8\") as f:          \n",
    "        f.write('Quote,Author,Tags\\n')\n",
    "\n",
    "        for container in containers:\n",
    "            Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "            Author = container.find('small',{'class' : 'author'}).text\n",
    "            Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "\n",
    "            f.write(Quote + ',' + Author + ',' + Tags + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = scrap_url + u\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navigator = page.findAll('div', {'class' : 'col-md-8'})\n",
    "navigat = navigator[1]\n",
    "u = navigat.nav.a['href']\n",
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://quotes.toscrape.com/page/2/'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "scrap_url = 'http://quotes.toscrape.com'\n",
    "urlClnt = urlopen(scrap_url)\n",
    "raw_html = urlClnt.read()\n",
    "urlClnt.close\n",
    "\n",
    "page = soup(raw_html, 'html.parser')\n",
    "containers = page.findAll('div', {'class' : 'quote'})\n",
    "container = containers[0]\n",
    "\n",
    "with open('test.csv', 'w') as f:\n",
    "    f.write('Quote,Author,Tags\\n')\n",
    "    \n",
    "    for container in containers:\n",
    "        Quote = container.find('span', {'class' : 'text'}).text.replace('“',r'').replace(',','').replace('”','')\n",
    "        Author = container.find('small',{'class' : 'author'}).text\n",
    "        Tags = container.find('meta', {'class': 'keywords'})['content'].replace('-',' ').replace(',','-')\n",
    "        \n",
    "        f.write(Quote + ',' + Author + ',' + Tags + '\\n')\n",
    "\n",
    "navigator = page.findAll('div', {'class' : 'col-md-8'})\n",
    "navigat = navigator[1]\n",
    "u = navigat.nav.a['href']\n",
    "# len(u)\n",
    "url = scrap_url + u\n",
    "url\n",
    "scrap_url = url\n",
    "scrap_url    # getting next page url while existing in current page. \n",
    "\n",
    "\n",
    "# What i want to do now:\n",
    "\n",
    "# I have got a new url in the 'scrap_url' variable in the last line. Now, I want to send the new 'scrap_url' \n",
    "# to the beginning 'scrap_url' and update the beginning 'scrap_url' with new value and \n",
    "# continue the whole process as long as we get different value for 'scrap_url'(at the end) and 'scrap_url'(at the beginning)\n",
    "\n",
    "# If we can loop over this process then we will use 'append' mode in file instead of 'write mode'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "navigator = page.findAll('div', {'class' : 'col-md-8'})\n",
    "navigat = navigator[1]\n",
    "u = navigat.nav.a['href']\n",
    "len(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://quotes.toscrape.com//page/2/'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = scrap_url + u\n",
    "url"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
